{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import KMeans\n",
    "from sklearn.ensemble import IsolationForest\n",
    "from sklearn.metrics import roc_auc_score, silhouette_score\n",
    "from sklearn.preprocessing import LabelBinarizer, StandardScaler\n",
    "from sklearn.model_selection import GridSearchCV, train_test_split\n",
    "from sklearn.svm import OneClassSVM\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from scipy.stats import skew, kurtosis\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = np.load('first_batch_multi_labels.npz')\n",
    "\n",
    "X = data['X']\n",
    "y = data['y']\n",
    "yy = data['yy']\n",
    "\n",
    "labels = yy[:, 1]\n",
    "labels\n",
    "\n",
    "df = pd.DataFrame(X, columns=['user', 'item', 'rating'])\n",
    "\n",
    "df_y = pd.DataFrame(yy, columns=['user', 'label'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AUC score for class 0 (test set): 0.7200000000000001\n",
      "AUC score for class 1 (test set): 0.6309523809523809\n",
      "AUC score for class 2 (test set): 0.7880952380952381\n",
      "Mean AUC (test set): 0.713015873015873\n"
     ]
    }
   ],
   "source": [
    "# Step 1: Feature Engineering\n",
    "df_user_features = df.groupby('user').agg(\n",
    "    mean_rating=('rating', 'mean'),\n",
    "    median_rating=('rating', 'median'),\n",
    "    std_rating=('rating', 'std'),\n",
    "    count_dislike=('rating', lambda x: (x == -10).sum()),\n",
    "    count_neutral=('rating', lambda x: (x == 0).sum()),\n",
    "    count_like=('rating', lambda x: (x == 10).sum()),\n",
    "    count_watched=('rating', lambda x: (x == 1).sum()),\n",
    "    total_interactions=('rating', 'count')\n",
    ")\n",
    "\n",
    "# Ratio features\n",
    "df_user_features['like_ratio'] = df_user_features['count_like'] / df_user_features['total_interactions']\n",
    "df_user_features['dislike_ratio'] = df_user_features['count_dislike'] / df_user_features['total_interactions']\n",
    "df_user_features['neutral_ratio'] = df_user_features['count_neutral'] / df_user_features['total_interactions']\n",
    "df_user_features['watched_ratio'] = df_user_features['count_watched'] / df_user_features['total_interactions']\n",
    "\n",
    "# Interaction patterns\n",
    "df_user_features['like_to_dislike_ratio'] = df_user_features['count_like'] / (df_user_features['count_dislike'] + 1)\n",
    "df_user_features['rating_variance'] = df.groupby('user')['rating'].var()\n",
    "\n",
    "# Distribution features\n",
    "df_user_features['rating_skew'] = df.groupby('user')['rating'].apply(lambda x: skew(x))\n",
    "df_user_features['rating_kurtosis'] = df.groupby('user')['rating'].apply(lambda x: kurtosis(x))\n",
    "\n",
    "# User behavior patterns\n",
    "df_user_features['rating_range'] = df.groupby('user')['rating'].apply(lambda x: x.max() - x.min())\n",
    "df_user_features['unique_items_ratio'] = df.groupby('user')['item'].nunique() / df_user_features['total_interactions']\n",
    "\n",
    "# Step 2: Merge user features with labels from yy\n",
    "df_labels = pd.DataFrame(yy, columns=['user', 'label'])\n",
    "df_merged = pd.merge(df_user_features.reset_index(), df_labels, on='user')\n",
    "\n",
    "# Step 3: Split the data into train and test sets (train on class 0 only, evaluate on all)\n",
    "train_df, test_df = train_test_split(df_merged, test_size=0.2, stratify=df_merged['label'], random_state=42)\n",
    "\n",
    "# Train set for class 0 only\n",
    "train_class_0 = train_df[train_df['label'] == 0]\n",
    "X_train_class_0 = train_class_0.drop(columns=['user', 'label'])\n",
    "\n",
    "# Test set for all classes (0, 1, and 2)\n",
    "X_test = test_df.drop(columns=['user', 'label'])\n",
    "y_test = test_df['label']\n",
    "\n",
    "# Standardize the features\n",
    "scaler = StandardScaler()\n",
    "X_scaled_train_0 = scaler.fit_transform(X_train_class_0)\n",
    "X_scaled_test = scaler.transform(X_test)\n",
    "\n",
    "# Step 4: Train One-Class SVM on class 0 (train set)\n",
    "oc_svm_class_0 = OneClassSVM(kernel='rbf', nu=0.1, gamma='scale')\n",
    "oc_svm_class_0.fit(X_scaled_train_0)\n",
    "\n",
    "# Step 5: Predict on the test set (class 0, 1, and 2)\n",
    "test_predictions = oc_svm_class_0.predict(X_scaled_test)\n",
    "# Map predictions to class labels (normal/inliers are class 0, anomalies are class 1 and 2)\n",
    "test_predicted_labels = np.where(test_predictions == -1, 1, 0)\n",
    "\n",
    "# Step 6: Compute AUC for each class on the test set\n",
    "\n",
    "# AUC for class 0 vs (class 1 and class 2)\n",
    "lb = LabelBinarizer()\n",
    "binarized_labels_0 = lb.fit_transform(np.where(y_test == 0, 1, 0))  # Class 0 vs rest\n",
    "auc_class_0 = roc_auc_score(binarized_labels_0, np.where(test_predicted_labels == 0, 1, 0))\n",
    "\n",
    "# AUC for class 1 vs (class 0 and class 2)\n",
    "binarized_labels_1 = lb.fit_transform(np.where(y_test == 1, 1, 0))  # Class 1 vs rest\n",
    "auc_class_1 = roc_auc_score(binarized_labels_1, np.where(test_predicted_labels == 1, 1, 0))\n",
    "\n",
    "# AUC for class 2 vs (class 0 and class 1)\n",
    "binarized_labels_2 = lb.fit_transform(np.where(y_test == 2, 1, 0))  # Class 2 vs rest\n",
    "auc_class_2 = roc_auc_score(binarized_labels_2, np.where(test_predicted_labels == 1, 1, 0))  # Anomalies as class 1\n",
    "\n",
    "# Output the AUC scores for each class\n",
    "print(f\"AUC score for class 0 (test set): {auc_class_0}\")\n",
    "print(f\"AUC score for class 1 (test set): {auc_class_1}\")\n",
    "print(f\"AUC score for class 2 (test set): {auc_class_2}\")\n",
    "\n",
    "# Mean AUC on the test set\n",
    "mean_auc_test = np.mean([auc_class_0, auc_class_1, auc_class_2])\n",
    "print(f\"Mean AUC (test set): {mean_auc_test}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
