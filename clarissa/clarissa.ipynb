{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn import metrics\n",
    "from sklearn.metrics import accuracy_score, roc_auc_score\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import matplotlib.pyplot as plt\n",
    "import xgboost as xgb\n",
    "from scipy.stats import kurtosis\n",
    "from sklearn.model_selection import train_test_split\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(filename):\n",
    "    data = np.load(filename)\n",
    "    X = data['X']\n",
    "    X = pd.DataFrame(X)\n",
    "    X.rename(columns={0:'user', 1:'item', 2:'rating'}, inplace=True)\n",
    "    X.sort_values(by=['user'], inplace=True)\n",
    "    \n",
    "    list_of_user_feature_data = []\n",
    "    for user_id in sorted(X.user.unique()):\n",
    "        user_feature_dict = dict()\n",
    "        user_feature_dict['user'] = user_id\n",
    "        \n",
    "        rating_interactions_df = X[X['user'] == user_id]\n",
    "        rating_interactions_df.loc[:, 'item'] = rating_interactions_df['item'].astype(str)\n",
    "        rating_interactions_dict = dict(zip(rating_interactions_df['item'], rating_interactions_df['rating']))\n",
    "        user_feature_dict.update(rating_interactions_dict)\n",
    "        \n",
    "        # manually engineer features\n",
    "        # get the ratings from user with user id == user_id\n",
    "        user_ratings = X[X['user'] == user_id]['rating']\n",
    "        user_feature_dict['sum_of_ratings'] = sum(user_ratings)\n",
    "        user_feature_dict['avg_of_ratings'] = np.mean(user_ratings)\n",
    "        user_feature_dict['total_num_ratings'] = len(user_ratings)\n",
    "        user_feature_dict['variance_of_ratings'] = np.var(user_ratings)\n",
    "        user_feature_dict['std_dev_of_ratings'] = np.std(user_ratings)\n",
    "        \n",
    "        user_feature_dict['count_positives'] = sum(rating > 0 for rating in user_ratings)\n",
    "        user_feature_dict['count_negatives'] = sum(rating < 0 for rating in user_ratings)\n",
    "        user_feature_dict['count_neutrals'] = sum(rating == 0 for rating in user_ratings)\n",
    "        \n",
    "        user_feature_dict['proportion_of_positives'] = user_feature_dict['count_positives'] / user_feature_dict['total_num_ratings']\n",
    "        user_feature_dict['proportion_of_negatives'] = user_feature_dict['count_negatives'] / user_feature_dict['total_num_ratings']\n",
    "        user_feature_dict['proportion_of_neutrals'] = user_feature_dict['count_neutrals'] / user_feature_dict['total_num_ratings']\n",
    "        user_feature_dict['outlier_count_kurtosis'] = kurtosis(user_ratings)\n",
    "        list_of_user_feature_data.append(user_feature_dict)\n",
    "        \n",
    "    ret_df = pd.DataFrame(list_of_user_feature_data)\n",
    "    \n",
    "    # features to create\n",
    "    X_item_attrs = X.groupby(['item']).agg({'rating': ['count', 'mean', 'median']})\n",
    "    \n",
    "    # flatten columns and rename\n",
    "    X_item_attrs.columns = [f\"{agg_type}_{agg_func}\" for agg_func, agg_type in X_item_attrs.columns]\n",
    "    X_item_attrs = X_item_attrs.rename(columns={'count_rating': 'rating_count', 'mean_rating': 'average_rating'}).reset_index()\n",
    "    \n",
    "    X['item_rating_count'] = X.apply(lambda row: X_item_attrs[X_item_attrs['item'] == row['item']]['rating_count'].item(), axis=1)\n",
    "    X['item_average_rating'] = X.apply(lambda row: X_item_attrs[X_item_attrs['item'] == row['item']]['average_rating'].item(), axis=1)\n",
    "    X['item_median_rating'] = X.apply(lambda row: X_item_attrs[X_item_attrs['item'] == row['item']]['median_rating'].item(), axis=1)\n",
    "\n",
    "    X['rating_deviation_from_item_average_rating'] = abs(X['rating'] - X['item_average_rating'])\n",
    "    X['rating_deviation_from_item_median_rating'] = abs(X['rating'] - X['item_median_rating'])\n",
    "    X['rating_above_item_median_rating'] = (X['rating'] - X['item_median_rating']).clip(0, None)\n",
    "\n",
    "    average_item = X.groupby(['user']).agg({'item_rating_count':'mean'})\n",
    "    average_item = average_item.rename(columns={'item_rating_count':'avg_item_rating_count'})\n",
    "\n",
    "    average_item['avg_item_avg_rating'] = X.groupby(['user']).agg({'item_average_rating':'mean'})\n",
    "\n",
    "    average_item['avg_item_median_rating'] = X.groupby(['user']).agg({'item_median_rating':'mean'})\n",
    "\n",
    "    average_item['avg_dev_from_item_median_rating'] = X.groupby(['user']).agg({'rating_deviation_from_item_median_rating':'mean'})\n",
    "    average_item['avg_rating_above_item_median_rating'] = X.groupby(['user']).agg({'rating_above_item_median_rating':'mean'})\n",
    "\n",
    "    average_item = average_item.reset_index()\n",
    "\n",
    "    ret_df = ret_df.merge(average_item, on=['user'], how='left')\n",
    "        \n",
    "    ret_df = ret_df.drop(columns=['avg_item_rating_count'])\n",
    "    \n",
    "    ret_df = ret_df.sort_values('user')\n",
    "    print('User order:', ret_df['user'])\n",
    "    ret_df = ret_df.drop(columns=['user'])\n",
    "    \n",
    "    return ret_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_training_data(train_filename):\n",
    "    X = preprocess(train_filename)\n",
    "    data=np.load(train_filename)\n",
    "    yy=data[\"yy\"]\n",
    "    yy=pd.DataFrame(yy)\n",
    "    yy.rename(columns={0:\"user\",1:\"label\"},inplace=True)\n",
    "    yy.sort_values(by='user', inplace=True)\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, yy['label'], test_size=0.2, random_state=42)\n",
    "    return X_train, X_test, y_train, y_test\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/xb/fc73t0kx10jdny5r9bhz_hf80000gn/T/ipykernel_60448/3147999301.py:14: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  rating_interactions_df.loc[:, 'item'] = rating_interactions_df['item'].astype(str)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "User order: 0          0\n",
      "1          1\n",
      "2          2\n",
      "3          3\n",
      "4          4\n",
      "        ... \n",
      "1095    1095\n",
      "1096    1096\n",
      "1097    1097\n",
      "1098    1098\n",
      "1099    1099\n",
      "Name: user, Length: 1100, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "train_filename = 'first_batch_multi_labels.npz'\n",
    "X_train, X_test, y_train, y_test = get_training_data(train_filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train.columns = X_train.columns.astype(str)\n",
    "X_test.columns = X_test.columns.astype(str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_copy = X_train.copy(deep=True)\n",
    "X_test_copy = X_test.copy(deep=True) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_copy_cols = X_train_copy.columns.tolist()\n",
    "X_test_copy_cols = X_test_copy.columns.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_cols = X_train.columns.tolist()\n",
    "\n",
    "X_test = X_test[X_train_cols]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(880, 1032) (220, 1032)\n",
      "float64 float64\n"
     ]
    }
   ],
   "source": [
    "print(X_train_scaled.shape, X_test_scaled.shape)\n",
    "print(X_train_scaled.dtype, X_test_scaled.dtype)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = StandardScaler()\n",
    "\n",
    "X_train.columns = X_train.columns.astype(str)\n",
    "X_test.columns = X_test.columns.astype(str)\n",
    "\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "dtrain = xgb.DMatrix(X_train_scaled, label=y_train)\n",
    "\n",
    "# Set parameters\n",
    "parameters = {\n",
    "    'max_depth': 2,\n",
    "    'eta': 0.3,\n",
    "    'objective': 'multi:softprob',\n",
    "    'eval_metric': 'auc',\n",
    "    'num_class': 3,\n",
    "}\n",
    "\n",
    "# Train \n",
    "num_round = 1000\n",
    "bst = xgb.train(parameters, dtrain, num_round, verbose_eval=False)\n",
    "\n",
    "dpred = xgb.DMatrix(X_test_scaled)\n",
    "\n",
    "y_pred_probs = bst.predict(dpred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_classes = np.argmax(y_pred_probs, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.95"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy = accuracy_score(y_test, y_pred_classes)\n",
    "accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.982252861350977"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "auc = roc_auc_score(y_test, y_pred_probs, multi_class='ovr')\n",
    "auc"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
