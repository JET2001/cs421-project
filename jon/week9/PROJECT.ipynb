{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3dc61b84",
   "metadata": {},
   "source": [
    "### CS 421 PROJECT"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38ade660",
   "metadata": {},
   "source": [
    "**Background & Objective**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ce15d29",
   "metadata": {},
   "source": [
    "In this project, you will be working with data extracted from famous recommender systems type datasets: you are provided with a large set of interactions between users (persons) and items (movies). Whenever a user \"interacts\" with an item, it watches the movie and gives a \"rating\". There are 4 possible ratings: \"dislike\", \"neutral\", \"like\", and \"watched\". The \"watched\" rating indicates that the user has rated the movie, but the specific rating is unknown (that means you know that the user has watched the movie, but you don't know whether they liked it)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e2fab5e",
   "metadata": {},
   "source": [
    "In this exercise, we will **not** be performing the recommendation task per se. Instead, we will identify *anomalous users*. In the dataset that you are provided with, some of the data was corrupted. Whilst most of the data comes from real life user-item interactions from a famous movie rating website, some \"users\" are anomalous: they were generated by me according to some undisclosed procedure. Furthermore, there are **two types of anomalies** with two different generation procedures."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "031c248f",
   "metadata": {},
   "source": [
    "**Data**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64df689c",
   "metadata": {},
   "source": [
    "You are provided with two data frames: the first one (\"X\") contains the interactions provided to you, and the second one (\"yy\") contains the labels for the users."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8fc60513",
   "metadata": {},
   "source": [
    "As you can see, the three columns in \"X\" correspond to the user ID, the item ID and the rating (encoded into numerical form). Thus, each row of \"X\" contains a single interaction. For instance, if the row \"142, 152, 10\" is present, this means that the user with ID 142 has given the movie 152 a positive rating of \"like\"."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72766a98",
   "metadata": {},
   "source": [
    "The table below shows what each numerical encoding of the rating corresponds to:\n",
    "\n",
    "| Rating in X    | Meaning     |\n",
    "| :------------- | :---------- |\n",
    "| -10            | dislike     |\n",
    "| 0              | neutral     |\n",
    "| 10             | like        |\n",
    "| 1              | watched     |"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8dc9801",
   "metadata": {},
   "source": [
    "The dataframe \"yy\" has two columns. In the first column we have the user IDs, whilst the second column contains the labels. A label of 0 denotes a natural user (coming from real life interactions), whilst a label of 1 or 2 indicates an anomaly generated by me. The anomalies with label 1 are generated with a different procedure from the anomalies with label 2. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64aca98f",
   "metadata": {},
   "source": [
    "For instance, if the labels matrix contains the line \"142, 1\", it means that ALL of the ratings given by the user with ID 142 are fake, and generated according to the first anomaly generation procedure. This means all lines in the dataframe \"ratings\" which start with the user ID 142 correspond to fake interactions. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df4ef3e7",
   "metadata": {},
   "source": [
    "#### Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "876ae8f7",
   "metadata": {},
   "source": [
    "Your task is to be able to classify unseen instances as either anomalies or non anomalies (guess whether they are real users or if they were generated by me). As well as indicate which anomaly type they belong to. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05f1af48",
   "metadata": {},
   "source": [
    "There are **far more** normal users than anomalies in the dataset, which makes this a very heavily **unbalanced dataset**. Thus, accuracy will not be a good measure of performance, since simply predicting that every user is normal will give good accuracy. Thus, we need to use some other evaluation metrics (see lecture notes from week 3). "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e059a16",
   "metadata": {},
   "source": [
    "THE **EVALUATION METRIC** is:  THE **AUC** (AREA UNDER CURVE) for each class (thus, there are three performance measures, one for each class). The main final metric to evaluate the ranking will be the average of the three.  This means your programs should return a **score** for each user and anomaly type combination. For instance, your model's prediction for user 1200 should consist of three scores $z_0,z_1,z_2$ corresponding to the normal class and the two anomalous classes respectively. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f42d14f",
   "metadata": {},
   "source": [
    "Every few weeks, we will evaluate the performance of each team (on a *test set with unseen labels* that I will provide) in terms of all three AUCs."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb35f65c",
   "metadata": {},
   "source": [
    "The difficulty implied by **the generation procedure of the anomalies MAY CHANGE as the project evolves: depending on how well the teams are doing, I may generate easier or harder anomalies**."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0065547d",
   "metadata": {},
   "source": [
    "**Deliverables**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22ac837a",
   "metadata": {},
   "source": [
    "Together with this file, you are provided with a first batch of labelled examples \"first_batch_multi_labels.npz\". You are also provided with the test samples to rank by the next round (without labels) in the file \"second_batch_multi.npz\".\n",
    "\n",
    "The **first round** will take place after recess (week 9): you must hand in your scores for the second batch before the **WEDNESDAY at NOON (15th of October)**. We will then look at the results together on the Thursday.  \n",
    "\n",
    "We will check everyone's performance in this way every week (once on  week 10, once on week 11 and once on week 12). "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "527121a4",
   "metadata": {},
   "source": [
    "To summarise, the project deliverables are as follows:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3cab22a",
   "metadata": {},
   "source": [
    "- Before every checkpoint's deadline, you need to submit **a `.npz` file** containing a Numpy array of size $\\text{number of test batch users} \\times 3$, where the value of each cell corresponds to the predicted score of the user (row) belonging to the anomaly type (column). The order of rows should correspond to the user IDs. For example, if the test batch contains users 1100-2200, scores for user 1100 should be the first row (row 0), scores for user 1101 should be the second row (row 1), and so on."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "080cd9af",
   "metadata": {},
   "source": [
    "- On Week 12-13 (schedule to be decided), you need to present your work in class. The presentation duration is **10 minutes** with 5 minutes of QA. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ff218b2",
   "metadata": {},
   "source": [
    "- On Week 12, you need to submit your **Jupyter Notebook** (with comments in Markdown) and the **slides** for your presentation. \n",
    "- On week 13 you need to submit your **final report**. The final report should be 2-3 pages long (consisting of problem statement, literature review, and motivation of algorithm design) with unlimited references/appendix."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31f3015c",
   "metadata": {},
   "source": [
    "Whilst performance (expressed in terms of AUC and your ranking compared to other teams) at **each of the check points** (weeks 9 to 12 inclusive) is an **important component** of your **final grade**, the **final report** and the detail of the various methods you will have tried will **also** be very **important**. Ideally, to get perfect marks (A+), you should try at least **two supervised methods** and **two unsupervised methods**, as well as be ranked the **best team** in terms of performance. \n",
    "\n",
    "\n",
    "In addition, I will be especially interested in your **reasoning**. Especially high marks will be awarded to any team that is able to **qualitatively describe** the difference between the two anomaly types. You are also encouraged to compute statistics related to each class and describe what is different about them. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e2f77c4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "data=np.load(\"first_batch_multi_labels.npz\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a49b0983",
   "metadata": {},
   "outputs": [],
   "source": [
    "X=data[\"X\"]\n",
    "y=data[\"yy\"]\n",
    "\n",
    "\n",
    "\n",
    "XX=pd.DataFrame(X)\n",
    "yy=pd.DataFrame(y)\n",
    "XX.rename(columns={0:\"user\",1:\"item\",2:\"rating\"},inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6cfa39c0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>user</th>\n",
       "      <th>item</th>\n",
       "      <th>rating</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1073</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1073</td>\n",
       "      <td>15</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1073</td>\n",
       "      <td>24</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1073</td>\n",
       "      <td>29</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1073</td>\n",
       "      <td>33</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   user  item  rating\n",
       "0  1073     3       1\n",
       "1  1073    15       1\n",
       "2  1073    24       1\n",
       "3  1073    29       1\n",
       "4  1073    33      10"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "XX.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "05ecb084",
   "metadata": {},
   "outputs": [],
   "source": [
    "yy.rename(columns={0:\"user\",1:\"label\"},inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7e9642b4",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>user</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   user  label\n",
       "0     0      0\n",
       "1     1      0\n",
       "2     2      0\n",
       "3     3      0\n",
       "4     4      0"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "yy.head()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
