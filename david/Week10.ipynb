{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3dc61b84",
   "metadata": {},
   "source": [
    "### CS 421 PROJECT"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38ade660",
   "metadata": {},
   "source": [
    "**Background & Objective**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ce15d29",
   "metadata": {},
   "source": [
    "In this project, you will be working with data extracted from famous recommender systems type datasets: you are provided with a large set of interactions between users (persons) and items (movies). Whenever a user \"interacts\" with an item, it watches the movie and gives a \"rating\". There are 4 possible ratings: \"dislike\", \"neutral\", \"like\", and \"watched\". The \"watched\" rating indicates that the user has rated the movie, but the specific rating is unknown (that means you know that the user has watched the movie, but you don't know whether they liked it)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e2fab5e",
   "metadata": {},
   "source": [
    "In this exercise, we will **not** be performing the recommendation task per se. Instead, we will identify *anomalous users*. In the dataset that you are provided with, some of the data was corrupted. Whilst most of the data comes from real life user-item interactions from a famous movie rating website, some \"users\" are anomalous: they were generated by me according to some undisclosed procedure. Furthermore, there are **two types of anomalies** with two different generation procedures."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "031c248f",
   "metadata": {},
   "source": [
    "**Data**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64df689c",
   "metadata": {},
   "source": [
    "You are provided with two data frames: the first one (\"X\") contains the interactions provided to you, and the second one (\"yy\") contains the labels for the users."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8fc60513",
   "metadata": {},
   "source": [
    "As you can see, the three columns in \"X\" correspond to the user ID, the item ID and the rating (encoded into numerical form). Thus, each row of \"X\" contains a single interaction. For instance, if the row \"142, 152, 10\" is present, this means that the user with ID 142 has given the movie 152 a positive rating of \"like\"."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72766a98",
   "metadata": {},
   "source": [
    "The table below shows what each numerical encoding of the rating corresponds to:\n",
    "\n",
    "| Rating in X    | Meaning     |\n",
    "| :------------- | :---------- |\n",
    "| -10            | dislike     |\n",
    "| 0              | neutral     |\n",
    "| 10             | like        |\n",
    "| 1              | watched     |"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8dc9801",
   "metadata": {},
   "source": [
    "The dataframe \"yy\" has two columns. In the first column we have the user IDs, whilst the second column contains the labels. A label of 0 denotes a natural user (coming from real life interactions), whilst a label of 1 or 2 indicates an anomaly generated by me. The anomalies with label 1 are generated with a different procedure from the anomalies with label 2. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64aca98f",
   "metadata": {},
   "source": [
    "For instance, if the labels matrix contains the line \"142, 1\", it means that ALL of the ratings given by the user with ID 142 are fake, and generated according to the first anomaly generation procedure. This means all lines in the dataframe \"ratings\" which start with the user ID 142 correspond to fake interactions. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df4ef3e7",
   "metadata": {},
   "source": [
    "#### Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "876ae8f7",
   "metadata": {},
   "source": [
    "Your task is to be able to classify unseen instances as either anomalies or non anomalies (guess whether they are real users or if they were generated by me). As well as indicate which anomaly type they belong to. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05f1af48",
   "metadata": {},
   "source": [
    "There are **far more** normal users than anomalies in the dataset, which makes this a very heavily **unbalanced dataset**. Thus, accuracy will not be a good measure of performance, since simply predicting that every user is normal will give good accuracy. Thus, we need to use some other evaluation metrics (see lecture notes from week 3). "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e059a16",
   "metadata": {},
   "source": [
    "THE **EVALUATION METRIC** is:  THE **AUC** (AREA UNDER CURVE) for each class (thus, there are three performance measures, one for each class). The main final metric to evaluate the ranking will be the average of the three.  This means your programs should return a **score** for each user and anomaly type combination. For instance, your model's prediction for user 1200 should consist of three scores $z_0,z_1,z_2$ corresponding to the normal class and the two anomalous classes respectively. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f42d14f",
   "metadata": {},
   "source": [
    "Every few weeks, we will evaluate the performance of each team (on a *test set with unseen labels* that I will provide) in terms of all three AUCs."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb35f65c",
   "metadata": {},
   "source": [
    "The difficulty implied by **the generation procedure of the anomalies MAY CHANGE as the project evolves: depending on how well the teams are doing, I may generate easier or harder anomalies**."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0065547d",
   "metadata": {},
   "source": [
    "**Deliverables**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22ac837a",
   "metadata": {},
   "source": [
    "Together with this file, you are provided with a first batch of labelled examples \"first_batch_multi_labels.npz\". You are also provided with the test samples to rank by the next round (without labels) in the file \"second_batch_multi.npz\".\n",
    "\n",
    "The **first round** will take place after recess (week 9): you must hand in your scores for the second batch before the **WEDNESDAY at NOON (16th of October)**. We will then look at the results together on the Thursday.  \n",
    "\n",
    "We will check everyone's performance in this way every week (once on  week 10, once on week 11 and once on week 12). "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "527121a4",
   "metadata": {},
   "source": [
    "To summarise, the project deliverables are as follows:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3cab22a",
   "metadata": {},
   "source": [
    "- Before every checkpoint's deadline, you need to submit **a `.npz` file** containing a Numpy array of size $\\text{number of test batch users} \\times 3$, where the value of each cell corresponds to the predicted score of the user (row) belonging to the anomaly type (column). The order of rows should correspond to the user IDs. For example, if the test batch contains users 1100-2200, scores for user 1100 should be the first row (row 0), scores for user 1101 should be the second row (row 1), and so on."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "080cd9af",
   "metadata": {},
   "source": [
    "- On Week 12-13 (schedule to be decided), you need to present your work in class. The presentation duration is **10 minutes** with 5 minutes of QA. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ff218b2",
   "metadata": {},
   "source": [
    "- On Week 12, you need to submit your **Jupyter Notebook** (with comments in Markdown) and the **slides** for your presentation. \n",
    "- On week 13 you need to submit your **final report**. The final report should be 2-3 pages long (consisting of problem statement, literature review, and motivation of algorithm design) with unlimited references/appendix."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31f3015c",
   "metadata": {},
   "source": [
    "Whilst performance (expressed in terms of AUC and your ranking compared to other teams) at **each of the check points** (weeks 9 to 12 inclusive) is an **important component** of your **final grade**, the **final report** and the detail of the various methods you will have tried will **also** be very **important**. Ideally, to get perfect marks (A+), you should try at least **two supervised methods** and **two unsupervised methods**, as well as be ranked the **best team** in terms of performance. \n",
    "\n",
    "\n",
    "In addition, I will be especially interested in your **reasoning**. Especially high marks will be awarded to any team that is able to **qualitatively describe** the difference between the two anomaly types. You are also encouraged to compute statistics related to each class and describe what is different about them. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e2f77c4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "from sklearn import linear_model, preprocessing\n",
    "from sklearn.metrics import roc_auc_score, confusion_matrix, roc_curve, auc\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "b1_data = np.load(\"first_batch_multi_labels.npz\")\n",
    "b2_data = np.load(\"second_batch_multi_labels.npz\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "a49b0983",
   "metadata": {},
   "outputs": [],
   "source": [
    "b1_X = b1_data[\"X\"]\n",
    "b1_y = b1_data[\"yy\"]\n",
    "b2_X = b2_data[\"X\"]\n",
    "b2_y = b2_data[\"yy\"]\n",
    "\n",
    "b1_XX = pd.DataFrame(b1_X)\n",
    "b1_yy = pd.DataFrame(b1_y)\n",
    "b2_XX = pd.DataFrame(b2_X)\n",
    "b2_yy = pd.DataFrame(b2_y)\n",
    "\n",
    "data_XX = pd.concat([b1_XX, b2_XX])\n",
    "data_YY = pd.concat([b1_yy, b2_yy])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "6cfa39c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_XX.head()\n",
    "data_XX.rename(columns={0:\"user\", 1:\"item_id\", 2:\"rating\"}, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "05ecb084",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_YY.rename(columns={0:\"user\",1:\"label\"},inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "7e9642b4",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>user</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   user  label\n",
       "0     0      0\n",
       "1     1      0\n",
       "2     2      0\n",
       "3     3      0\n",
       "4     4      0"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_YY.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "41410eef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: numpy in c:\\users\\david\\anaconda3\\lib\\site-packages (1.23.5)\n",
      "Requirement already satisfied: scipy in c:\\users\\david\\anaconda3\\lib\\site-packages (1.7.3)\n",
      "Requirement already satisfied: numba in c:\\users\\david\\anaconda3\\lib\\site-packages (0.55.1)\n",
      "Collecting numpy\n",
      "  Using cached numpy-1.22.4-cp39-cp39-win_amd64.whl (14.7 MB)\n",
      "Requirement already satisfied: llvmlite<0.39,>=0.38.0rc1 in c:\\users\\david\\anaconda3\\lib\\site-packages (from numba) (0.38.0)\n",
      "Requirement already satisfied: setuptools in c:\\users\\david\\anaconda3\\lib\\site-packages (from numba) (61.2.0)\n",
      "  Downloading numpy-1.21.6-cp39-cp39-win_amd64.whl (14.0 MB)\n",
      "Installing collected packages: numpy\n",
      "  Attempting uninstall: numpy\n",
      "    Found existing installation: numpy 1.23.5\n",
      "    Uninstalling numpy-1.23.5:\n",
      "      Successfully uninstalled numpy-1.23.5\n",
      "Successfully installed numpy-1.21.6\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Ignoring invalid distribution -rotobuf (c:\\users\\david\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -rotobuf (c:\\users\\david\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -rotobuf (c:\\users\\david\\anaconda3\\lib\\site-packages)\n",
      "    WARNING: Ignoring invalid distribution -rotobuf (c:\\users\\david\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -rotobuf (c:\\users\\david\\anaconda3\\lib\\site-packages)\n",
      "ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "daal4py 2021.5.0 requires daal==2021.4.0, which is not installed.\n",
      "tensorflow-intel 2.15.0 requires ml-dtypes~=0.2.0, but you have ml-dtypes 0.4.1 which is incompatible.\n",
      "tensorflow-intel 2.15.0 requires numpy<2.0.0,>=1.23.5, but you have numpy 1.21.6 which is incompatible.\n",
      "WARNING: Ignoring invalid distribution -rotobuf (c:\\users\\david\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -rotobuf (c:\\users\\david\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -rotobuf (c:\\users\\david\\anaconda3\\lib\\site-packages)\n"
     ]
    }
   ],
   "source": [
    "!pip install numpy scipy numba\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "b8583457",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>user</th>\n",
       "      <th>like_ratio_user</th>\n",
       "      <th>dislike_ratio_user</th>\n",
       "      <th>mean_rating_user</th>\n",
       "      <th>median_rating_user</th>\n",
       "      <th>like_ratio_item</th>\n",
       "      <th>dislike_ratio_item</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1073</td>\n",
       "      <td>0.259259</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>2.962963</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.222222</td>\n",
       "      <td>0.116466</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1073</td>\n",
       "      <td>0.259259</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>2.962963</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.416144</td>\n",
       "      <td>0.038401</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1073</td>\n",
       "      <td>0.259259</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>2.962963</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.353858</td>\n",
       "      <td>0.060795</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1073</td>\n",
       "      <td>0.259259</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>2.962963</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.305503</td>\n",
       "      <td>0.049336</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1073</td>\n",
       "      <td>0.259259</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>2.962963</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.241463</td>\n",
       "      <td>0.080488</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>353519</th>\n",
       "      <td>1660</td>\n",
       "      <td>0.197531</td>\n",
       "      <td>0.166667</td>\n",
       "      <td>0.851852</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.400236</td>\n",
       "      <td>0.043684</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>353520</th>\n",
       "      <td>1660</td>\n",
       "      <td>0.197531</td>\n",
       "      <td>0.166667</td>\n",
       "      <td>0.851852</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.355401</td>\n",
       "      <td>0.055749</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>353521</th>\n",
       "      <td>1660</td>\n",
       "      <td>0.197531</td>\n",
       "      <td>0.166667</td>\n",
       "      <td>0.851852</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.238434</td>\n",
       "      <td>0.085409</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>353522</th>\n",
       "      <td>1660</td>\n",
       "      <td>0.197531</td>\n",
       "      <td>0.166667</td>\n",
       "      <td>0.851852</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.337434</td>\n",
       "      <td>0.054482</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>353523</th>\n",
       "      <td>1660</td>\n",
       "      <td>0.197531</td>\n",
       "      <td>0.166667</td>\n",
       "      <td>0.851852</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.421222</td>\n",
       "      <td>0.045016</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>353524 rows Ã— 8 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        user  like_ratio_user  dislike_ratio_user  mean_rating_user  \\\n",
       "0       1073         0.259259            0.000000          2.962963   \n",
       "1       1073         0.259259            0.000000          2.962963   \n",
       "2       1073         0.259259            0.000000          2.962963   \n",
       "3       1073         0.259259            0.000000          2.962963   \n",
       "4       1073         0.259259            0.000000          2.962963   \n",
       "...      ...              ...                 ...               ...   \n",
       "353519  1660         0.197531            0.166667          0.851852   \n",
       "353520  1660         0.197531            0.166667          0.851852   \n",
       "353521  1660         0.197531            0.166667          0.851852   \n",
       "353522  1660         0.197531            0.166667          0.851852   \n",
       "353523  1660         0.197531            0.166667          0.851852   \n",
       "\n",
       "        median_rating_user  like_ratio_item  dislike_ratio_item  label  \n",
       "0                      1.0         0.222222            0.116466      0  \n",
       "1                      1.0         0.416144            0.038401      0  \n",
       "2                      1.0         0.353858            0.060795      0  \n",
       "3                      1.0         0.305503            0.049336      0  \n",
       "4                      1.0         0.241463            0.080488      0  \n",
       "...                    ...              ...                 ...    ...  \n",
       "353519                 1.0         0.400236            0.043684      2  \n",
       "353520                 1.0         0.355401            0.055749      2  \n",
       "353521                 1.0         0.238434            0.085409      2  \n",
       "353522                 1.0         0.337434            0.054482      2  \n",
       "353523                 1.0         0.421222            0.045016      2  \n",
       "\n",
       "[353524 rows x 8 columns]"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Merge the interactions data (X) with the labels data (yy) on the 'user' column\n",
    "merged_data = pd.merge(data_XX, data_YY, on='user')\n",
    "\n",
    "# Function to calculate the ratio of a particular rating for a user\n",
    "def user_rating_ratio(df, rating_value):\n",
    "    return df.groupby('user')['rating'].apply(lambda x: (x == rating_value).mean())\n",
    "\n",
    "# Function to calculate the ratio of a particular rating for an item\n",
    "def item_rating_ratio(df, rating_value):\n",
    "    return df.groupby('item_id')['rating'].apply(lambda x: (x == rating_value).mean())\n",
    "\n",
    "# Add user-based features\n",
    "merged_data['like_ratio_user'] = merged_data['user'].map(user_rating_ratio(merged_data, 10))\n",
    "merged_data['dislike_ratio_user'] = merged_data['user'].map(user_rating_ratio(merged_data, -10))\n",
    "merged_data['mean_rating_user'] = merged_data.groupby('user')['rating'].transform('mean')\n",
    "merged_data['median_rating_user'] = merged_data.groupby('user')['rating'].transform('median')\n",
    "\n",
    "# Add item-based features\n",
    "merged_data['like_ratio_item'] = merged_data['item_id'].map(item_rating_ratio(merged_data, 10))\n",
    "merged_data['dislike_ratio_item'] = merged_data['item_id'].map(item_rating_ratio(merged_data, -10))\n",
    "\n",
    "# Keep the 'label' as it is from previous merging\n",
    "features = merged_data[['user', 'like_ratio_user', 'dislike_ratio_user', 'mean_rating_user', \n",
    "                        'median_rating_user', 'like_ratio_item', 'dislike_ratio_item', 'label']]\n",
    "features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "cd4cf87a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1, loss = 0.22509208\n",
      "Validation score: 0.962379\n",
      "Iteration 2, loss = 0.07702514\n",
      "Validation score: 0.981189\n",
      "Iteration 3, loss = 0.05996857\n",
      "Validation score: 0.982745\n",
      "Iteration 4, loss = 0.05511781\n",
      "Validation score: 0.983700\n",
      "Iteration 5, loss = 0.05028678\n",
      "Validation score: 0.985432\n",
      "Iteration 6, loss = 0.04649387\n",
      "Validation score: 0.986422\n",
      "Iteration 7, loss = 0.04427035\n",
      "Validation score: 0.986316\n",
      "Iteration 8, loss = 0.04131991\n",
      "Validation score: 0.987660\n",
      "Iteration 9, loss = 0.03896515\n",
      "Validation score: 0.988296\n",
      "Iteration 10, loss = 0.03786821\n",
      "Validation score: 0.983488\n",
      "Iteration 11, loss = 0.03545316\n",
      "Validation score: 0.988190\n",
      "Iteration 12, loss = 0.03393842\n",
      "Validation score: 0.989923\n",
      "Iteration 13, loss = 0.03220766\n",
      "Validation score: 0.991443\n",
      "Iteration 14, loss = 0.03096851\n",
      "Validation score: 0.990559\n",
      "Iteration 15, loss = 0.02952529\n",
      "Validation score: 0.992398\n",
      "Iteration 16, loss = 0.02733703\n",
      "Validation score: 0.990135\n",
      "Iteration 17, loss = 0.02701565\n",
      "Validation score: 0.990984\n",
      "Iteration 18, loss = 0.02562877\n",
      "Validation score: 0.991408\n",
      "Iteration 19, loss = 0.02528462\n",
      "Validation score: 0.992327\n",
      "Iteration 20, loss = 0.02335458\n",
      "Validation score: 0.988827\n",
      "Iteration 21, loss = 0.02420190\n",
      "Validation score: 0.989746\n",
      "Iteration 22, loss = 0.02143353\n",
      "Validation score: 0.987448\n",
      "Iteration 23, loss = 0.02036491\n",
      "Validation score: 0.993989\n",
      "Iteration 24, loss = 0.01997972\n",
      "Validation score: 0.995297\n",
      "Iteration 25, loss = 0.01916805\n",
      "Validation score: 0.993388\n",
      "Iteration 26, loss = 0.01861798\n",
      "Validation score: 0.994131\n",
      "Iteration 27, loss = 0.01789752\n",
      "Validation score: 0.994838\n",
      "Iteration 28, loss = 0.01709881\n",
      "Validation score: 0.995828\n",
      "Iteration 29, loss = 0.01691994\n",
      "Validation score: 0.995757\n",
      "Iteration 30, loss = 0.01544016\n",
      "Validation score: 0.996429\n",
      "Iteration 31, loss = 0.01583444\n",
      "Validation score: 0.995156\n",
      "Iteration 32, loss = 0.01519920\n",
      "Validation score: 0.996075\n",
      "Iteration 33, loss = 0.01388592\n",
      "Validation score: 0.995722\n",
      "Iteration 34, loss = 0.01360556\n",
      "Validation score: 0.996111\n",
      "Iteration 35, loss = 0.01235894\n",
      "Validation score: 0.997242\n",
      "Iteration 36, loss = 0.01510428\n",
      "Validation score: 0.996005\n",
      "Iteration 37, loss = 0.01162991\n",
      "Validation score: 0.996995\n",
      "Iteration 38, loss = 0.01109440\n",
      "Validation score: 0.997277\n",
      "Iteration 39, loss = 0.01143836\n",
      "Validation score: 0.994484\n",
      "Iteration 40, loss = 0.01080483\n",
      "Validation score: 0.998126\n",
      "Iteration 41, loss = 0.01068190\n",
      "Validation score: 0.997490\n",
      "Iteration 42, loss = 0.01034578\n",
      "Validation score: 0.998621\n",
      "Iteration 43, loss = 0.01030034\n",
      "Validation score: 0.998161\n",
      "Iteration 44, loss = 0.00952723\n",
      "Validation score: 0.997808\n",
      "Iteration 45, loss = 0.00874507\n",
      "Validation score: 0.998727\n",
      "Iteration 46, loss = 0.00982158\n",
      "Validation score: 0.995439\n",
      "Iteration 47, loss = 0.01064422\n",
      "Validation score: 0.998550\n",
      "Iteration 48, loss = 0.00863150\n",
      "Validation score: 0.997843\n",
      "Iteration 49, loss = 0.00815202\n",
      "Validation score: 0.998091\n",
      "Iteration 50, loss = 0.00765905\n",
      "Validation score: 0.997383\n",
      "Iteration 51, loss = 0.00911468\n",
      "Validation score: 0.995545\n",
      "Iteration 52, loss = 0.00637404\n",
      "Validation score: 0.998444\n",
      "Iteration 53, loss = 0.00903225\n",
      "Validation score: 0.997808\n",
      "Iteration 54, loss = 0.00710773\n",
      "Validation score: 0.997348\n",
      "Iteration 55, loss = 0.00919250\n",
      "Validation score: 0.998975\n",
      "Iteration 56, loss = 0.00660735\n",
      "Validation score: 0.997065\n",
      "Iteration 57, loss = 0.00742210\n",
      "Validation score: 0.996323\n",
      "Iteration 58, loss = 0.00696484\n",
      "Validation score: 0.998197\n",
      "Iteration 59, loss = 0.00791714\n",
      "Validation score: 0.998727\n",
      "Iteration 60, loss = 0.00691309\n",
      "Validation score: 0.998197\n",
      "Iteration 61, loss = 0.00663942\n",
      "Validation score: 0.999222\n",
      "Iteration 62, loss = 0.00698046\n",
      "Validation score: 0.996924\n",
      "Iteration 63, loss = 0.00972695\n",
      "Validation score: 0.997277\n",
      "Iteration 64, loss = 0.00522819\n",
      "Validation score: 0.998586\n",
      "Iteration 65, loss = 0.00533207\n",
      "Validation score: 0.989569\n",
      "Iteration 66, loss = 0.00680676\n",
      "Validation score: 0.997772\n",
      "Iteration 67, loss = 0.00627373\n",
      "Validation score: 0.999293\n",
      "Iteration 68, loss = 0.00601089\n",
      "Validation score: 0.997136\n",
      "Iteration 69, loss = 0.00542564\n",
      "Validation score: 0.998798\n",
      "Iteration 70, loss = 0.00651653\n",
      "Validation score: 0.998621\n",
      "Iteration 71, loss = 0.00685208\n",
      "Validation score: 0.998055\n",
      "Iteration 72, loss = 0.00741151\n",
      "Validation score: 0.999682\n",
      "Iteration 73, loss = 0.00360610\n",
      "Validation score: 0.995191\n",
      "Iteration 74, loss = 0.00668651\n",
      "Validation score: 0.999328\n",
      "Iteration 75, loss = 0.00552394\n",
      "Validation score: 0.999788\n",
      "Iteration 76, loss = 0.00636506\n",
      "Validation score: 0.999328\n",
      "Iteration 77, loss = 0.00430915\n",
      "Validation score: 0.998798\n",
      "Iteration 78, loss = 0.00584740\n",
      "Validation score: 0.992752\n",
      "Iteration 79, loss = 0.00597376\n",
      "Validation score: 0.999611\n",
      "Iteration 80, loss = 0.00412524\n",
      "Validation score: 0.994555\n",
      "Iteration 81, loss = 0.00532245\n",
      "Validation score: 0.999682\n",
      "Iteration 82, loss = 0.00676164\n",
      "Validation score: 0.999364\n",
      "Iteration 83, loss = 0.00431446\n",
      "Validation score: 0.999470\n",
      "Iteration 84, loss = 0.00662008\n",
      "Validation score: 0.999364\n",
      "Iteration 85, loss = 0.00345171\n",
      "Validation score: 0.999505\n",
      "Iteration 86, loss = 0.00473130\n",
      "Validation score: 0.990665\n",
      "Validation score did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "\n",
      "Classification Report:\n",
      "                precision    recall  f1-score   support\n",
      "\n",
      "        Normal       1.00      1.00      1.00     65072\n",
      "Anomaly Type 1       1.00      1.00      1.00      2226\n",
      "Anomaly Type 2       1.00      1.00      1.00      3407\n",
      "\n",
      "      accuracy                           1.00     70705\n",
      "     macro avg       1.00      1.00      1.00     70705\n",
      "  weighted avg       1.00      1.00      1.00     70705\n",
      "\n",
      "AUC for Normal: 0.9999992879563774\n",
      "AUC for Anomaly Type 1: 0.9999996851101459\n",
      "AUC for Anomaly Type 2: 0.9999999214948381\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.metrics import classification_report, roc_auc_score\n",
    "import numpy as np\n",
    "\n",
    "# Assuming 'features' is the dataframe with the engineered features and 'label' is the target\n",
    "X = features.drop(columns=['label'])\n",
    "y = features['label']\n",
    "\n",
    "# Scale the feature data\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X)\n",
    "\n",
    "# Split the data into training and test sets (80% training, 20% testing)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_scaled, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Build the MLPClassifier model\n",
    "mlp = MLPClassifier(hidden_layer_sizes=(64, 32, 16), max_iter=50, verbose=True, random_state=42, early_stopping=True)\n",
    "\n",
    "\n",
    "# Train the model\n",
    "mlp.fit(X_train, y_train)\n",
    "\n",
    "# Predict the labels for the test set\n",
    "y_pred = mlp.predict(X_test)\n",
    "\n",
    "# Predict the probabilities for the test set\n",
    "y_pred_proba = mlp.predict_proba(X_test)\n",
    "\n",
    "# Classification report for Precision, Recall, F1-Score\n",
    "print(\"\\nClassification Report:\")\n",
    "print(classification_report(y_test, y_pred, target_names=['Normal', 'Anomaly Type 1', 'Anomaly Type 2']))\n",
    "\n",
    "# AUC for each individual class (one-vs-rest)\n",
    "y_test_onehot = pd.get_dummies(y_test)  # One-hot encode the true labels\n",
    "for i, class_name in enumerate(['Normal', 'Anomaly Type 1', 'Anomaly Type 2']):\n",
    "    class_auc = roc_auc_score(y_test_onehot.iloc[:, i], y_pred_proba[:, i])\n",
    "    print(f'AUC for {class_name}: {class_auc}')\n",
    "\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
